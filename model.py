import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class LinearQNet(nn.Module):

    def __init__(self, inputSize, hiddenSize, outputSize):
        super().__init__()

        self.linear1 = nn.Linear(inputSize, hiddenSize)
        self.linear2 = nn.Linear(hiddenSize, outputSize)

    def forward(self, X):
        out = self.linear1(X)
        out = F.relu(out)
        out = self.linear2(out)

        return out

    class QTrainner:

        def __init__(self, model, lr, gamma):
            self.model = model
            self.lr = lr
            self.gamma = gamma

            self.optimizer = optim.Adam(model.parameters(), self.lr)
            self.lossFunction = nn.MSELoss()

        def trainStep(self, state, action, reward, newState, done):

            stateTensor = torch.tensor(state, dtype=torch.float)
            actionTensor = torch.tensor(action, dtype=torch.long)
            rewardTensor = torch.tensor(reward, dtype=torch.float)
            newStateTensor = torch.tensor(newState, dtype=torch.float)

            if len(stateTensor.shape) == 1:
                stateTensor = torch.unsqueeze(stateTensor, 0)
                newStateTensor = torch.unsqueeze(newStateTensor, 0)
                actionTensor = torch.unsqueeze(actionTensor, 0)
                rewardTensor = torch.unsqueeze(rewardTensor, 0)
                done = (done,)

            # 1. predicted q values with current state
            prediction = self.model(stateTensor)

            # Q_new = 2. reward + gamma * max(next predicted q value) -> only do this if not done
            target = prediction.clone()

            for i in range(len(done)):
                Qnew = rewardTensor[i]

                if not done[i]:
                    Qnew = rewardTensor[i] + self.gamma * torch.max(self.model(newStateTensor[i]))

                target[i][torch.argmax(actionTensor).item()] = Qnew

            self.optimizer.zero_grad()
            loss = self.lossFunction(target, prediction)
            loss.backward()

            self.optimizer.step()


class QTrainner:

    def __init__(self, model, lr, gamma):
        self.model = model
        self.lr = lr
        self.gamma = gamma

        self.optimizer = optim.Adam(model.parameters(), self.lr)
        self.lossFunction = nn.MSELoss()

    def trainStep(self, state, action, reward, newState, done):

        stateTensor = torch.tensor(state, dtype=torch.float)
        actionTensor = torch.tensor(action, dtype=torch.long)
        rewardTensor = torch.tensor(reward, dtype=torch.float)
        newStateTensor = torch.tensor(newState, dtype=torch.float)

        if len(stateTensor.shape) == 1:
            stateTensor = torch.unsqueeze(stateTensor, 0)
            newStateTensor = torch.unsqueeze(newStateTensor, 0)
            actionTensor = torch.unsqueeze(actionTensor, 0)
            rewardTensor = torch.unsqueeze(rewardTensor, 0)
            done = (done, )

        # 1. predicted q values with current state
        prediction = self.model(stateTensor)

        # Q_new = 2. reward + gamma * max(next predicted q value) -> only do this if not done
        target = prediction.clone()

        for i in range(len(done)):
            Qnew = rewardTensor[i]

            if not done[i]:
                Qnew = rewardTensor[i] + self.gamma * torch.max(self.model(newStateTensor[i]))

            target[i][torch.argmax(actionTensor).item()] = Qnew


        self.optimizer.zero_grad()
        loss = self.lossFunction(target, prediction)
        loss.backward()

        self.optimizer.step()